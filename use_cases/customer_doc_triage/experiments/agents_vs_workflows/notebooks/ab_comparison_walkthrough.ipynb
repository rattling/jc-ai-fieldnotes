{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84db0fe0",
   "metadata": {},
   "source": [
    "# Agents vs Workflows â€” A/B Walkthrough\n",
    "\n",
    "This notebook runs both orchestration modes on the same sample documents and compares outputs.\n",
    "\n",
    "Use this for exploratory analysis and narrative reporting. Use the CLI for repeatable operational runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6b02030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experiment_dir': '/home/john/repos/jc-ai-fieldnotes/use_cases/customer_doc_triage/experiments/agents_vs_workflows', 'samples_exists': True}\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from agents_vs_workflows.workflow.pipeline import run_workflow\n",
    "from agents_vs_workflows.agent.pipeline import run_agentic\n",
    "from agents_vs_workflows.eval.metrics import score\n",
    "\n",
    "\n",
    "def resolve_experiment_dir() -> Path:\n",
    "    cwd = Path.cwd().resolve()\n",
    "\n",
    "    direct_candidate = cwd / \"use_cases\" / \"customer_doc_triage\" / \"experiments\" / \"agents_vs_workflows\"\n",
    "    if (direct_candidate / \"data\" / \"samples.jsonl\").exists():\n",
    "        return direct_candidate\n",
    "\n",
    "    if (cwd / \"data\" / \"samples.jsonl\").exists() and cwd.name == \"agents_vs_workflows\":\n",
    "        return cwd\n",
    "\n",
    "    for base in [cwd, *cwd.parents]:\n",
    "        candidate = base / \"use_cases\" / \"customer_doc_triage\" / \"experiments\" / \"agents_vs_workflows\"\n",
    "        if (candidate / \"data\" / \"samples.jsonl\").exists():\n",
    "            return candidate\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not locate use_cases/customer_doc_triage/experiments/agents_vs_workflows/data/samples.jsonl from current working directory.\"\n",
    "    )\n",
    "\n",
    "\n",
    "EXPERIMENT_DIR = resolve_experiment_dir()\n",
    "SAMPLES_PATH = EXPERIMENT_DIR / \"data\" / \"samples.jsonl\"\n",
    "GOLD_PATH = EXPERIMENT_DIR / \"data\" / \"gold.jsonl\"\n",
    "print({\"experiment_dir\": str(EXPERIMENT_DIR), \"samples_exists\": SAMPLES_PATH.exists()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6b8339e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 200, 200)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_jsonl(path: Path, limit: int | None = None):\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    with path.open('r', encoding='utf-8') as handle:\n",
    "\n",
    "        for line in handle:\n",
    "\n",
    "            line = line.strip()\n",
    "\n",
    "            if not line:\n",
    "\n",
    "                continue\n",
    "\n",
    "            rows.append(json.loads(line))\n",
    "\n",
    "            if limit is not None and len(rows) >= limit:\n",
    "\n",
    "                break\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "\n",
    "samples = read_jsonl(SAMPLES_PATH)\n",
    "\n",
    "samples_preview = samples[:12]\n",
    "\n",
    "gold_rows = read_jsonl(GOLD_PATH)\n",
    "\n",
    "gold_by_id = {row['doc_id']: row for row in gold_rows}\n",
    "\n",
    "len(samples_preview), len(samples), len(gold_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb5b53a",
   "metadata": {},
   "source": [
    "## Run both modes on identical inputs\n",
    "\n",
    "The key comparison principle is fixed inputs + shared output schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d9af2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'workflow_cases': 200, 'agent_cases': 200, 'preview_cases': 12}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "import agents_vs_workflows.workflow.pipeline as workflow_pipeline\n",
    "\n",
    "import agents_vs_workflows.agent.tools as agent_tools\n",
    "\n",
    "import agents_vs_workflows.agent.planner as agent_planner\n",
    "\n",
    "import agents_vs_workflows.agent.pipeline as agent_pipeline\n",
    "\n",
    "\n",
    "\n",
    "importlib.reload(workflow_pipeline)\n",
    "\n",
    "importlib.reload(agent_tools)\n",
    "\n",
    "importlib.reload(agent_planner)\n",
    "\n",
    "importlib.reload(agent_pipeline)\n",
    "\n",
    "\n",
    "\n",
    "run_workflow = workflow_pipeline.run_workflow\n",
    "\n",
    "run_agentic = agent_pipeline.run_agentic\n",
    "\n",
    "\n",
    "\n",
    "workflow_predictions = []\n",
    "\n",
    "agent_predictions = []\n",
    "\n",
    "\n",
    "\n",
    "for sample in samples:\n",
    "\n",
    "    workflow_predictions.append(run_workflow(sample, max_retries=1).model_dump())\n",
    "\n",
    "    agent_predictions.append(run_agentic(sample, max_tool_calls=6, timeout_ms=2000).model_dump())\n",
    "\n",
    "\n",
    "\n",
    "{\n",
    "\n",
    "    'workflow_cases': len(workflow_predictions),\n",
    "\n",
    "    'agent_cases': len(agent_predictions),\n",
    "\n",
    "    'preview_cases': len(samples_preview),\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4ac4a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'workflow': {'doc_id': 'DOC-0001',\n",
       "   'doc_type': 'security_questionnaire',\n",
       "   'priority': 'P2',\n",
       "   'queue': 'compliance_ops',\n",
       "   'escalate': False,\n",
       "   'missing': ['required_due_date'],\n",
       "   'tool_calls': 0,\n",
       "   'elapsed_ms': 0},\n",
       "  'agent': {'doc_id': 'DOC-0001',\n",
       "   'doc_type': 'security_questionnaire',\n",
       "   'priority': 'P2',\n",
       "   'queue': 'compliance_ops',\n",
       "   'escalate': False,\n",
       "   'missing': ['required_due_date'],\n",
       "   'tool_calls': 3,\n",
       "   'elapsed_ms': 0}},\n",
       " {'workflow': {'doc_id': 'DOC-0002',\n",
       "   'doc_type': 'billing_dispute',\n",
       "   'priority': 'P1',\n",
       "   'queue': 'billing_ops',\n",
       "   'escalate': False,\n",
       "   'missing': ['invoice_id'],\n",
       "   'tool_calls': 0,\n",
       "   'elapsed_ms': 0},\n",
       "  'agent': {'doc_id': 'DOC-0002',\n",
       "   'doc_type': 'billing_dispute',\n",
       "   'priority': 'P1',\n",
       "   'queue': 'billing_ops',\n",
       "   'escalate': False,\n",
       "   'missing': ['invoice_id'],\n",
       "   'tool_calls': 3,\n",
       "   'elapsed_ms': 0}},\n",
       " {'workflow': {'doc_id': 'DOC-0003',\n",
       "   'doc_type': 'incident_report',\n",
       "   'priority': 'P1',\n",
       "   'queue': 'support_incident',\n",
       "   'escalate': True,\n",
       "   'missing': ['request_id_examples'],\n",
       "   'tool_calls': 0,\n",
       "   'elapsed_ms': 0},\n",
       "  'agent': {'doc_id': 'DOC-0003',\n",
       "   'doc_type': 'incident_report',\n",
       "   'priority': 'P1',\n",
       "   'queue': 'support_incident',\n",
       "   'escalate': True,\n",
       "   'missing': ['request_id_examples'],\n",
       "   'tool_calls': 4,\n",
       "   'elapsed_ms': 0}},\n",
       " {'workflow': {'doc_id': 'DOC-0004',\n",
       "   'doc_type': 'billing_dispute',\n",
       "   'priority': 'P2',\n",
       "   'queue': 'billing_ops',\n",
       "   'escalate': False,\n",
       "   'missing': ['invoice_id'],\n",
       "   'tool_calls': 0,\n",
       "   'elapsed_ms': 0},\n",
       "  'agent': {'doc_id': 'DOC-0004',\n",
       "   'doc_type': 'billing_dispute',\n",
       "   'priority': 'P2',\n",
       "   'queue': 'billing_ops',\n",
       "   'escalate': False,\n",
       "   'missing': ['invoice_id'],\n",
       "   'tool_calls': 3,\n",
       "   'elapsed_ms': 0}},\n",
       " {'workflow': {'doc_id': 'DOC-0005',\n",
       "   'doc_type': 'incident_report',\n",
       "   'priority': 'P1',\n",
       "   'queue': 'support_incident',\n",
       "   'escalate': True,\n",
       "   'missing': ['request_id_examples'],\n",
       "   'tool_calls': 0,\n",
       "   'elapsed_ms': 0},\n",
       "  'agent': {'doc_id': 'DOC-0005',\n",
       "   'doc_type': 'incident_report',\n",
       "   'priority': 'P1',\n",
       "   'queue': 'support_incident',\n",
       "   'escalate': True,\n",
       "   'missing': ['request_id_examples'],\n",
       "   'tool_calls': 4,\n",
       "   'elapsed_ms': 0}}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compact(pred):\n",
    "\n",
    "    trace = pred.get('decision_trace', {})\n",
    "\n",
    "    return {\n",
    "\n",
    "        'doc_id': pred['doc_id'],\n",
    "\n",
    "        'doc_type': pred['doc_type'],\n",
    "\n",
    "        'priority': pred['priority'],\n",
    "\n",
    "        'queue': pred['recommended_queue'],\n",
    "\n",
    "        'escalate': pred['escalate'],\n",
    "\n",
    "        'missing': pred['required_missing_fields'],\n",
    "\n",
    "        'tool_calls': trace.get('tool_calls', 0),\n",
    "\n",
    "        'elapsed_ms': trace.get('elapsed_ms', 0),\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "preview_ids = {row['doc_id'] for row in samples_preview}\n",
    "\n",
    "preview_workflow = [row for row in workflow_predictions if row['doc_id'] in preview_ids]\n",
    "\n",
    "preview_agent = [row for row in agent_predictions if row['doc_id'] in preview_ids]\n",
    "\n",
    "\n",
    "\n",
    "[\n",
    "\n",
    "    {'workflow': compact(w), 'agent': compact(a)}\n",
    "\n",
    "    for w, a in zip(preview_workflow[:5], preview_agent[:5])\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b24fe08",
   "metadata": {},
   "source": [
    "## Compare aggregate metrics (sample subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c79e4abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'workflow': {'doc_type_accuracy': 0.915,\n",
       "  'queue_accuracy': 0.915,\n",
       "  'escalation_precision': 0.2125,\n",
       "  'escalation_recall': 1.0,\n",
       "  'missing_field_recall': 0.965,\n",
       "  'avg_elapsed_ms': 0.0,\n",
       "  'avg_tool_calls': 0.0,\n",
       "  'distinct_step_patterns': 1},\n",
       " 'agent': {'doc_type_accuracy': 1.0,\n",
       "  'queue_accuracy': 1.0,\n",
       "  'escalation_precision': 0.25757575757575757,\n",
       "  'escalation_recall': 1.0,\n",
       "  'missing_field_recall': 1.0,\n",
       "  'avg_elapsed_ms': 0.0,\n",
       "  'avg_tool_calls': 3.465,\n",
       "  'distinct_step_patterns': 3}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "import agents_vs_workflows.eval.metrics as eval_metrics\n",
    "\n",
    "\n",
    "\n",
    "importlib.reload(eval_metrics)\n",
    "\n",
    "score = eval_metrics.score\n",
    "\n",
    "\n",
    "\n",
    "workflow_metrics = score(workflow_predictions, gold_by_id)\n",
    "\n",
    "agent_metrics = score(agent_predictions, gold_by_id)\n",
    "\n",
    "\n",
    "\n",
    "{\n",
    "\n",
    "    'workflow': {\n",
    "\n",
    "        'doc_type_accuracy': workflow_metrics['doc_type_accuracy'],\n",
    "\n",
    "        'queue_accuracy': workflow_metrics['queue_accuracy'],\n",
    "\n",
    "        'escalation_precision': workflow_metrics['escalation_precision'],\n",
    "\n",
    "        'escalation_recall': workflow_metrics['escalation_recall'],\n",
    "\n",
    "        'missing_field_recall': workflow_metrics['missing_field_recall'],\n",
    "\n",
    "        'avg_elapsed_ms': workflow_metrics['avg_elapsed_ms'],\n",
    "\n",
    "        'avg_tool_calls': workflow_metrics['avg_tool_calls'],\n",
    "\n",
    "        'distinct_step_patterns': workflow_metrics['distinct_step_patterns'],\n",
    "\n",
    "    },\n",
    "\n",
    "    'agent': {\n",
    "\n",
    "        'doc_type_accuracy': agent_metrics['doc_type_accuracy'],\n",
    "\n",
    "        'queue_accuracy': agent_metrics['queue_accuracy'],\n",
    "\n",
    "        'escalation_precision': agent_metrics['escalation_precision'],\n",
    "\n",
    "        'escalation_recall': agent_metrics['escalation_recall'],\n",
    "\n",
    "        'missing_field_recall': agent_metrics['missing_field_recall'],\n",
    "\n",
    "        'avg_elapsed_ms': agent_metrics['avg_elapsed_ms'],\n",
    "\n",
    "        'avg_tool_calls': agent_metrics['avg_tool_calls'],\n",
    "\n",
    "        'distinct_step_patterns': agent_metrics['distinct_step_patterns'],\n",
    "\n",
    "    },\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277f85df",
   "metadata": {},
   "source": [
    "## Suggested interpretation prompts\n",
    "\n",
    "\n",
    "\n",
    "- Where does agent mode improve recall on missing-field detection?\n",
    "\n",
    "- Are escalation precision/recall shifts acceptable for ops policy?\n",
    "\n",
    "- How much latency/tool-call overhead appears in agent mode?\n",
    "\n",
    "- Which doc types show the largest quality delta?\n",
    "\n",
    "- If quality metrics are equal, compare structural behavior (`distinct_step_patterns`, `avg_tool_calls`) to verify dynamic vs fixed orchestration differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d277c5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccbd19d9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dba59f3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed675109",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5effb1dc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a29ae9f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jc-ai-fieldnotes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
